{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise Housing\n",
    "A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia.\n",
    "\n",
    " \n",
    "\n",
    "The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n",
    "\n",
    " \n",
    "\n",
    "The company wants to know:\n",
    "\n",
    "* Which variables are significant in predicting the price of a house, and\n",
    "\n",
    "* How well those variables describe the price of a house.\n",
    "\n",
    " \n",
    "\n",
    "Also, determine the optimal value of lambda for ridge and lasso regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Goal \n",
    " \n",
    "\n",
    "We are required to model the price of houses with the available independent variables. This model will then be used by the management to understand how exactly the prices vary with the variables. They can accordingly manipulate the strategy of the firm and concentrate on areas that will yield high returns. Further, the model will be a good way for management to understand the pricing dynamics of a new market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignement has been divided in the follow parts:\n",
    "* Data preparation\n",
    "* Data cleaning\n",
    "* EDA\n",
    "* Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing neccary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data\n",
    "main_df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head() #For a glimpse of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 1460 records and 81 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cans see there are are many null objects in the data set provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(main_df.isnull().sum()/len(main_df)*100).sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are lots of missing data for PoolQC, MiscFeature, Alley and Fence. We can safely remove these columns or we can impute the NA values with some appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's impute data\n",
    "#For PoolQC, NA simple means None. So let's impute that way\n",
    "df[\"PoolQC\"] = df[\"PoolQC\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, for MiscFeature, Alley, Fence, FireplaceQu\n",
    "df[\"MiscFeature\"] = df[\"MiscFeature\"].fillna(\"None\")\n",
    "df[\"Alley\"] = df[\"Alley\"].fillna(\"None\")\n",
    "df[\"FireplaceQu\"] = df[\"FireplaceQu\"].fillna(\"None\")\n",
    "df[\"Fence\"] = df[\"Fence\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"GarageType\"] = df[\"GarageType\"].fillna(\"None\")\n",
    "df[\"GarageFinish\"] = df[\"GarageFinish\"].fillna(\"None\")\n",
    "df[\"GarageQual\"] = df[\"GarageQual\"].fillna(\"None\")\n",
    "df[\"GarageCond\"] = df[\"GarageCond\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For some of the numerical attributes like GarageArea and GarageCars, we can replace nulls with 0\n",
    "df[\"GarageArea\"] = df[\"GarageArea\"].fillna(0)\n",
    "df[\"GarageCars\"] = df[\"GarageCars\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LotFrontage, we can impute median LotFrontage of all Neighbourhood\n",
    "df[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n",
    "    lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GarageYrBlt can be imputed by meadia\n",
    "#df[\"GarageYrBlt\"]=df[\"GarageYrBlt\"].astype(str)\n",
    "df[\"GarageYrBlt\"]=df[\"GarageYrBlt\"].fillna(df[\"GarageYrBlt\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['PoolQC','MiscFeature','Alley','Fence'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Let's see how much missing values are still left\n",
    "(df.isnull().sum()/len(df)*100).sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again for BsmtFinType2, BsmtExposure, BsmtQual, BsmtFinType1 and BsmtCond can be imputed with None\n",
    "df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].fillna(\"None\")\n",
    "df[\"BsmtExposure\"] = df[\"BsmtExposure\"].fillna(\"None\")\n",
    "df[\"BsmtQual\"] = df[\"BsmtQual\"].fillna(\"None\")\n",
    "df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].fillna(\"None\")\n",
    "df[\"BsmtCond\"] = df[\"BsmtCond\"].fillna(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For MasVnrType and MasVnrArea, we can impute with None and 0 repectively\n",
    "df[\"MasVnrType\"] = df[\"MasVnrType\"].fillna(\"None\")\n",
    "df[\"MasVnrArea\"] = df[\"MasVnrArea\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Electrical column let's impute it with the largest occured value, i.e. Mode.\n",
    "df[\"Electrical\"] = df[\"Electrical\"].fillna(df[\"Electrical\"].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also make some new columns which will give more information about the data set. Such as Total Area, Bathroom and Year Average\n",
    "\n",
    "df['TotalArea'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea'] +df['GarageArea']\n",
    "df['Bathrooms'] = df['FullBath'] + df['HalfBath']*0.5 \n",
    "df['Year average']= (df['YearRemodAdd']+df['YearBuilt'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look on the heat map that shows the correlation between the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(12,9))\n",
    "sns.heatmap(df.corr(), square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above Heat map we can say:\n",
    "* Garage Year built and Year Built of the house are highly correlated.\n",
    "* Total Rooms Above Grade and Above Grade living area are highly correlated.\n",
    "* Garage Area and Garage Cars are highly correlated.\n",
    "* Sale Price and Overall Quality are highly correlated.\n",
    "* Sale Price and Grade living area are highly correlated.\n",
    "* Garage Year built and Enclosed Porch are negatively correlated\n",
    "* Garage Year built and Overall Condition are negatively correlated\n",
    "* Year Built and Enclosed Porch are negatively correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Numerical columns\n",
    "df_num = df.select_dtypes(include=['float64', 'int64'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have plotted the pair plot here but for 38 attributes, it will be a computational heavy deal. So we will select some of the attributes and see the pattern with Sales (target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(df_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num.columns\n",
    "df_num =df_num.drop('Id', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding outliers using scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since we are focused on finding the SalePrice, we will try to find the pattern with other attributes.\n",
    "plt.scatter(df.GrLivArea,df.SalePrice)\n",
    "plt.xlabel('Living Area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see the points in right hand side are not following the trend. Like for more `Living Area` the `Sale Price` is low. We can see there are some outliers here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.TotalBsmtSF,df.SalePrice)\n",
    "plt.xlabel('Basement Area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['1stFlrSF'],df.SalePrice)\n",
    "plt.xlabel('First Floor Area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.MasVnrArea,df.SalePrice)\n",
    "plt.xlabel('Masonry veneer area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some outliers using Box Plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,50))\n",
    "i=0\n",
    "for col in df_num.columns:\n",
    "    i = i+1\n",
    "    plt.subplot(30,4,i)\n",
    "    sns.boxplot(y=col,data=df)\n",
    "    #plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's treat the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#We are using z score approach to removing the outliers.\n",
    "from scipy import stats\n",
    "df_outliers_treated = df.copy()\n",
    "df_outliers_treated=df_outliers_treated[(np.abs(stats.zscore(df_outliers_treated[df_num.columns])) < 3.5).all(axis=1)]\n",
    "df_outliers_treated.dropna(axis=0,inplace=True)\n",
    "df_outliers_treated.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For z score = 3 it was dropping nearly 500 rows and the outliers are less. So we have increased the z score value to 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_outliers_treated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plots after the outlier treatement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Since we are focused on finding the SalePrice, we will try to find the pattern with other attributes.\n",
    "plt.scatter(df.GrLivArea,df.SalePrice)\n",
    "plt.xlabel('Living Area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.TotalBsmtSF,df.SalePrice)\n",
    "plt.xlabel('Basement Area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(df['1stFlrSF'],df.SalePrice)\n",
    "plt.xlabel('First Floor Area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.MasVnrArea,df.SalePrice)\n",
    "plt.xlabel('Masonry veneer area')\n",
    "plt.ylabel('Sale Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there are no outliers anymore. And for the number of rows removed is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We removed ',main_df.shape[0]- df.shape[0],'outliers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_num = df.select_dtypes(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data = []  #Makking array to store the column names with unique values and counts\n",
    "for col in df_non_num.columns:\n",
    "    col_data.append([col,df_non_num[col].unique(),df_non_num[col].nunique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_num_col_data = pd.DataFrame(col_data, columns=['ColName','UniqueValues','UniqueCounts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_num_col_data.sort_values(by='UniqueCounts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that `PoolQC` and `Utilities` have only one unique value. So we can say there are no variation for these columns and we can remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['PoolQC','Utilities'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create dummy variable for each non numerical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df, prefix_sep='_', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.drop('Id', inplace=True,axis = 1)\n",
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 238 columns after creating dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Skew in the target (Sale Price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(df_dummies.SalePrice)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the histogram is slighly shifted to the left hand side. And this need to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(df_dummies.SalePrice))  #To rectify the the skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =np.log(df_dummies.pop('SalePrice')) #It can be considered as the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using Robust Scaler\n",
    "Since we have seen there are Skew in the data and the outliers as well.\n",
    "Robust Scaler is a better choice for this scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler= RobustScaler()\n",
    "cols = df_dummies.columns\n",
    "X = pd.DataFrame(scaler.fit_transform(df_dummies))\n",
    "X.columns = cols\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data in Test and Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size=0.7,\n",
    "                                                    test_size = 0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building and Evaluation\n",
    "Here we use `Ridge` and `Lasso` regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge=Ridge()\n",
    "params= {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}#{'alpha':[x for x in range(1,101)]}\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "model_cv = GridSearchCV(estimator = ridge, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "model_cv.fit(X_train, y_train) \n",
    "print(\"The best value of Alpha is: \",model_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's see accuracy for different values of alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.0001,0.001,1,5,10]\n",
    "for alp in alphas:\n",
    "    ridge_mod = Ridge(alpha=alp)\n",
    "    ridge_mod.fit(X_train, y_train)\n",
    "    #ridge_mod_test = Ridge(X_test,y_test)\n",
    "    print(\"Accuracy score for train set for alpha \", alp,\" is \",ridge_mod.score(X_train, y_train), \" and Accuracy score for test is \",ridge_mod.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the scores are pretty good for alpha = 10. (But for 7 we may get better result )\n",
    "\n",
    "For alpha = 0.0001, 0.001 and 1 the test accuracy score is bad and train score is very good. Which is a sign of over fitting of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot showing the variation of test and train score with various alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "#cv_results.head()\n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can the Root mean square error values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sklm\n",
    "y_pred_train=ridge_mod.predict(X_train)\n",
    "y_pred_test=ridge_mod.predict(X_test)\n",
    "import math\n",
    "print('Root Mean Square Error train = ' + str(round(math.sqrt(sklm.mean_squared_error(y_train, y_pred_train)),2)))\n",
    "print('Root Mean Square Error test = ' + str(round(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test)),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "coefs = pd.Series(ridge_mod.coef_, index = X.columns)\n",
    "\n",
    "imp_coefs = pd.concat([coefs.sort_values().head(10),\n",
    "                     coefs.sort_values().tail(10)])\n",
    "imp_coefs.plot(kind = \"barh\")\n",
    "plt.title(\"Features importance in the Ridge Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that Ridge Regularisation is not used to select the columns but we have picked the top 10 and bottom 10 columns from the Ridge coeficient.\n",
    "\n",
    "So following are some inferences from the the plot:\n",
    "* Overall quality of the house is an important factor for the Sale Price\n",
    "* Living Area is accountable for the high Sale Price\n",
    "* If the Neighbourhood is Crawford, the price is expected to be more\n",
    "* If the Neighbourhood is Edwards, the prices will get very low.\n",
    "* If the Pool quality is good also, the prices are expected to get low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "\n",
    "# cross validation\n",
    "model_cv = GridSearchCV(estimator = lasso, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "\n",
    "model_cv.fit(X_train, y_train) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite dificult to see the alpha value in the plot. But from we can find the alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "parameters= {'alpha':[0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "lasso=Lasso()\n",
    "lasso_reg=GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=5)\n",
    "lasso_reg.fit(X,y)\n",
    "print('The best value of Alpha is: ',lasso_reg.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the Lasso model with alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha =0.001\n",
    "\n",
    "lasso_mod = Lasso(alpha=alpha)\n",
    "        \n",
    "lasso_mod.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are coeficients generated by the lasso model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_mod.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize it for our better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "coefs = pd.Series(lasso_mod.coef_, index = X.columns)\n",
    "\n",
    "imp_coefs = pd.concat([coefs.sort_values().head(10),\n",
    "                     coefs.sort_values().tail(10)])\n",
    "imp_coefs.plot(kind = \"barh\")\n",
    "plt.title(\"Features importance in the Lasso Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lasso Model selected\",sum(coefs != 0), \"important features and dropped the other\", sum(coefs == 0),\" features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the Lasso Model selection is used for selecting features as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can the Root mean square error values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train=lasso_mod.predict(X_train)\n",
    "y_pred_test=lasso_mod.predict(X_test)\n",
    "import math\n",
    "print('Root Mean Square Error train = ' + str(round(math.sqrt(sklm.mean_squared_error(y_train, y_pred_train)),2)))\n",
    "print('Root Mean Square Error test = ' + str(round(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test)),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the RMSE is same as for Ridge as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding from the analysis\n",
    "* The property with high Total Area will cost more and hence the profit margin can be better.\n",
    "* If the Overall Quality is good the price of the property will be more.\n",
    "* Property with Neighbourhood Crawfor, adds up the Sale Price.\n",
    "* For Sale Type New, the price is expected to be more.\n",
    "* Property with good condition will be sold for more price.\n",
    "* The absence of the Fireplace has shown the decline of the price of the property. `Surprise` can invest in Fireplace and increase the price of the property.\n",
    "* `Surprise` should be stay away from Townhouse building type.\n",
    "* It should also avoid the property with neighbourhood Edwards.\n",
    "* `Surprise` can invest some in heating system and enhance the quality and then the property can be sold in a better price."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
